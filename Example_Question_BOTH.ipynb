{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 4.1. Suppose a wheel is rotating on an axle, the total moment of inirtia being $J$, \n",
    "and it is required to bring the system to rest by applying a braking torque $u(t)$. \n",
    "The equation of motion is\n",
    "\n",
    "\\begin{equation}\n",
    "    J\\frac{dx}{dt} = u \n",
    "\\end{equation}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Now take $ t \\in [0,1] $ and B.C. are $ x(0) = 10 $ and $ x(1) = 0 $. To solve this problem we will construct trail solution for $x$ and for $u$ with different Neural Networks for each of them and then optimize them simultaneously. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ODE: \n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{dx}{dt} = \\frac{u(t)}{J}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Where\n",
    "\n",
    "- $ x(t) $ is angular velocity.\n",
    "- $ J $ is total moment of inertia.\n",
    "- $ u(t) $ is breaking torque.\n",
    "\n",
    "\n",
    "Let\n",
    "\n",
    "\\begin{equation}\n",
    "    \\begin{aligned}\n",
    "        t &\\in [0,1] \\\\\n",
    "        J &= 10 \\\\\n",
    "        x(0) &= 10 \\\\\n",
    "        x(1) &= 0\n",
    "    \\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Exact (actual) solution of above ODE is:\n",
    "\n",
    "\\begin{equation}\n",
    "    u_a(t_0) = \\frac{-J x(t_0)}{t_1-t_0}\n",
    "\\end{equation}\n",
    "\n",
    "for above  condtions,\n",
    "\n",
    "\\begin{equation}\n",
    "    \\begin{aligned}\n",
    "        u_a(0) &= -100 \\\\\n",
    "        u_a(1) &= 0\n",
    "    \\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Trail solution is of the form:\n",
    "\n",
    "\\begin{equation}\n",
    "    x(t)_{tr} = A(1-t) + Bt + t (1-t) \\frac{1}{J} u(t)_{tr} + t (1-t) N_x(t,p) \n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "    u(t)_{tr} = (1-t) C + t N_u(t,p)\n",
    "\\end{equation}\n",
    "\n",
    "Where,\n",
    "\n",
    "\\begin{equation}\n",
    "    \\begin{aligned}\n",
    "        A &= x(0) = 10 \\\\\n",
    "        B &= x(1) = 0 \\\\\n",
    "        C &= u_a(0) = -100 \\\\\n",
    "    \\end{aligned}\n",
    "    \\tag{5}\n",
    "\\end{equation}\n",
    "\n",
    "Therefore we can write,\n",
    "\n",
    "\\begin{equation}\n",
    "    x(t)_{tr} = \\frac{(1-t)}{J}[J A + t[(1-t) C + t N_u + J N_x]]\n",
    "\\end{equation}\n",
    "\n",
    "So for the above equation the Loss can be calculated using following equation,\n",
    "\n",
    "\\begin{equation}\n",
    "    L = \\sum \\left[ J\\frac{d x(t)_{tr}}{dt} - u_{tr} \\right]\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib_inline.backend_inline import set_matplotlib_formats\n",
    "\n",
    "set_matplotlib_formats(\"svg\")\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.Hidden_layer = nn.Linear(in_features=1, out_features=10)\n",
    "        self.Output_layer = nn.Linear(in_features=10, out_features=1, bias=False)\n",
    "        self.Sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.Hidden_layer(x)\n",
    "        x = self.Sigmoid(x)\n",
    "        output = self.Output_layer(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetwork(\n",
       "  (Hidden_layer): Linear(in_features=1, out_features=10, bias=True)\n",
       "  (Output_layer): Linear(in_features=10, out_features=1, bias=False)\n",
       "  (Sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NeuralNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size=1, hidden_size=1, num_layers=1, batch_first=True)\n",
    "        # self.output_layer = nn.Linear(10, 1, bias=False) # if hidden_size=10\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(-1)  # (batch, seq_len, input_size)\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        # output = self.output_layer(lstm_out) # if hidden_size=10\n",
    "        output = lstm_out\n",
    "        return output.squeeze(-1)  # Remove last dimension for consistency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM_Network(\n",
       "  (lstm): LSTM(1, 1, batch_first=True)\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LSTM_Network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def u_exact(J, x_0, t):\n",
    "    return -(J * x_0) / (t[-1] - t[0])\n",
    "\n",
    "\n",
    "def x_trial(t, N_x, x_0):\n",
    "    A = x_0\n",
    "    return (1 - t) * A + (1 - t) * t * N_x(t)\n",
    "\n",
    "\n",
    "def u_trail(N_u, t, x_0, J):\n",
    "    C = u_exact(J, x_0, t)\n",
    "    return C + t * N_u(t)\n",
    "\n",
    "\n",
    "def loss_1(t, J, models, x_0):\n",
    "    N_x = models[\"N_x\"]\n",
    "    N_u = models[\"N_u\"]\n",
    "\n",
    "    t.requires_grad = True\n",
    "    x_t = x_trial(t, N_x, x_0)\n",
    "    dx_dt = torch.autograd.grad(x_t.sum(), t, create_graph=True)[0]\n",
    "    u_tr = u_trail(N_u, t, x_0, J)\n",
    "\n",
    "    G = ((J * dx_dt) - u_tr) ** 2\n",
    "\n",
    "    return torch.sum(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = optim.Adam(\n",
    "#         [param for model_params in all_parms for param in model_params], lr=0.001\n",
    "#     ) ## 30000 epochs, 2e-4 mse\n",
    "\n",
    "# optimizer = optim.SGD(\n",
    "#         [param for model_params in all_parms for param in model_params], lr=0.01, momentum=0.9\n",
    "#     ) ## nan values\n",
    "\n",
    "# optimizer = torch.optim.LBFGS(\n",
    "#         [param for model_params in all_parms for param in model_params]\n",
    "#     ) ## 20 epochs, variable mse\n",
    "# optimizer = optim.RMSprop(\n",
    "#         [param for model_params in all_parms for param in model_params],\n",
    "#         lr=0.01,\n",
    "#         alpha=0.99,\n",
    "#         eps=1e-08,\n",
    "#         weight_decay=0,\n",
    "#         momentum=0.9,\n",
    "#         centered=False,\n",
    "#     ) ## 10000 epochs 1e-5 mse\n",
    "\n",
    "# optimizer = optim.Adagrad(\n",
    "#     [param for model_params in all_parms for param in model_params], lr=0.01\n",
    "# )  ## 1000 epochs 1e-3 mse\n",
    "\n",
    "# optimizer = optim.AdamW(\n",
    "#         [param for model_params in all_parms for param in model_params], lr=0.01\n",
    "#     ) ## 20000 epochs 2e-4 mse\n",
    "\n",
    "# optimizer = optim.Adadelta(\n",
    "#         [param for model_params in all_parms for param in model_params], lr=0.1\n",
    "#     ) ## 5000 epochs 1e-2 mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Optimize(t, J, epochs, models, x_0):\n",
    "    all_parms = [model.parameters() for model in models.values()]\n",
    "\n",
    "    optimizer = optim.LBFGS(\n",
    "        [param for model_params in all_parms for param in model_params]\n",
    "    )\n",
    "\n",
    "    def train():\n",
    "        with torch.backends.cudnn.flags(enabled=False):\n",
    "            optimizer.zero_grad()\n",
    "            loss = loss_1(t, J, models, x_0)\n",
    "            loss.backward()\n",
    "        return loss\n",
    "\n",
    "    for i in tqdm(range(epochs)):\n",
    "        optimizer.step(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE_Calculate(t, J, models, x_0):\n",
    "    N_u = models[\"N_u\"]\n",
    "    with torch.no_grad():\n",
    "        u_tr = u_trail(N_u, t, x_0, J).detach().cpu()\n",
    "        u_ex = u_exact(J, x_0, t).detach().cpu()\n",
    "\n",
    "    print(u_tr)\n",
    "    print(u_ex)\n",
    "    mse = torch.mean((u_tr - u_ex) ** 2)\n",
    "    print(f\"MSE: {mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 24.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-200.0000],\n",
      "        [-200.0000],\n",
      "        [-200.0000],\n",
      "        [-200.0000],\n",
      "        [-200.0000],\n",
      "        [-200.0000],\n",
      "        [-200.0000],\n",
      "        [-200.0000],\n",
      "        [-200.0000],\n",
      "        [-200.0000],\n",
      "        [-200.0000],\n",
      "        [-200.0000],\n",
      "        [-200.0000],\n",
      "        [-200.0000],\n",
      "        [-200.0000],\n",
      "        [-200.0000],\n",
      "        [-200.0000],\n",
      "        [-200.0000],\n",
      "        [-200.0000],\n",
      "        [-200.0000],\n",
      "        [-200.0000],\n",
      "        [-200.0000],\n",
      "        [-200.0000],\n",
      "        [-200.0000],\n",
      "        [-200.0000],\n",
      "        [-200.0000],\n",
      "        [-200.0000],\n",
      "        [-200.0000],\n",
      "        [-200.0000],\n",
      "        [-200.0000],\n",
      "        [-200.0000],\n",
      "        [-200.0000],\n",
      "        [-200.0000],\n",
      "        [-200.0000],\n",
      "        [-200.0000],\n",
      "        [-200.0000],\n",
      "        [-200.0000],\n",
      "        [-200.0000],\n",
      "        [-200.0000],\n",
      "        [-200.0000],\n",
      "        [-200.0000],\n",
      "        [-200.0000],\n",
      "        [-200.0000],\n",
      "        [-200.0000],\n",
      "        [-200.0000],\n",
      "        [-200.0000],\n",
      "        [-200.0000],\n",
      "        [-200.0000],\n",
      "        [-200.0000],\n",
      "        [-200.0000],\n",
      "        [-200.0000],\n",
      "        [-200.0000],\n",
      "        [-200.0000],\n",
      "        [-200.0000],\n",
      "        [-200.0000],\n",
      "        [-200.0000],\n",
      "        [-200.0000],\n",
      "        [-200.0000],\n",
      "        [-200.0000],\n",
      "        [-200.0000],\n",
      "        [-200.0000],\n",
      "        [-200.0000],\n",
      "        [-200.0000],\n",
      "        [-200.0000],\n",
      "        [-200.0000],\n",
      "        [-200.0000],\n",
      "        [-200.0000],\n",
      "        [-200.0000],\n",
      "        [-200.0000],\n",
      "        [-200.0000],\n",
      "        [-200.0000],\n",
      "        [-200.0000],\n",
      "        [-200.0000],\n",
      "        [-200.0000],\n",
      "        [-200.0000],\n",
      "        [-200.0000],\n",
      "        [-200.0000],\n",
      "        [-200.0000],\n",
      "        [-200.0000],\n",
      "        [-200.0000],\n",
      "        [-200.0000],\n",
      "        [-200.0000],\n",
      "        [-200.0000],\n",
      "        [-200.0000],\n",
      "        [-200.0000],\n",
      "        [-200.0000],\n",
      "        [-200.0000],\n",
      "        [-200.0000],\n",
      "        [-200.0000],\n",
      "        [-200.0000],\n",
      "        [-200.0000],\n",
      "        [-200.0000],\n",
      "        [-200.0000],\n",
      "        [-200.0000],\n",
      "        [-200.0000],\n",
      "        [-200.0000],\n",
      "        [-200.0000],\n",
      "        [-200.0000],\n",
      "        [-200.0000],\n",
      "        [-200.0000]])\n",
      "tensor([-200.])\n",
      "MSE: 5.355104901183516e-11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "J = torch.tensor(20).to(device=device)  # J = 20\n",
    "x_0 = torch.tensor(10).to(device=device)  # x_0 = 10\n",
    "\n",
    "### u_exact = -J*x_0 = -200\n",
    "\n",
    "T = torch.linspace(start=0, end=1, steps=100).unsqueeze(1).to(device=device)\n",
    "\n",
    "# # Create models\n",
    "# N_u = NeuralNetwork().to(device=device)\n",
    "# N_x = NeuralNetwork().to(device=device)\n",
    "\n",
    "# # Create LSTM models\n",
    "N_u = LSTM_Network().to(device=device)\n",
    "N_x = LSTM_Network().to(device=device)\n",
    "\n",
    "models = {\n",
    "    \"N_u\": N_u,\n",
    "    \"N_x\": N_x,\n",
    "}\n",
    "\n",
    "Optimize(T, J, EPOCHS, models, x_0)\n",
    "MSE_Calculate(T, J, models, x_0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1])\n",
      "torch.Size([10, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "T = torch.linspace(start=0, end=1, steps=10).unsqueeze(1).to(device=device)\n",
    "print(T.shape)\n",
    "print(T.unsqueeze(-1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "##----> This cell is for exporting the model to onnx format\n",
    "\n",
    "# # import torch\n",
    "\n",
    "# # model = LSTM_Network().to(device)\n",
    "# # dummy_input = torch.randn(5, 10).to(device)  # example input\n",
    "# # torch.onnx.export(\n",
    "# #     model,\n",
    "# #     dummy_input,\n",
    "# #     \"lstm_model.onnx\",\n",
    "# #     input_names=[\"input\"],\n",
    "# #     output_names=[\"output\"],\n",
    "# #     dynamic_axes={\n",
    "# #         \"input\": {0: \"batch_size\", 1: \"seq_len\"},\n",
    "# #         \"output\": {0: \"batch_size\", 1: \"seq_len\"},\n",
    "# #     },\n",
    "# # )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
